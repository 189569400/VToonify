{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/williamyang1991/DualStyleGAN/blob/master/notebooks/inference_playground.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code is mainly modified from [pixel2style2pixel](https://github.com/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uuviq3qQkUFy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.chdir('../')\n",
    "CODE_DIR = 'VToonify'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ6XEmlHlXbk"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/williamyang1991/VToonify.git $CODE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force \n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23baccYQlU9E"
   },
   "outputs": [],
   "source": [
    "os.chdir(f'./{CODE_DIR}')\n",
    "MODEL_DIR = os.path.join(os.path.dirname(os.getcwd()), CODE_DIR, 'checkpoint')\n",
    "DATA_DIR = os.path.join(os.path.dirname(os.getcwd()), CODE_DIR, 'data')\n",
    "OUT_DIR = os.path.join(os.path.dirname(os.getcwd()), CODE_DIR, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d13v7In0kTJn"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from model.vtoonify import VToonify\n",
    "from model.bisenet.model import BiSeNet\n",
    "from model.encoder.align_all_parallel import align_face\n",
    "from util import save_image, load_image, visualize, load_psp_standalone, get_video_crop_parameter, tensor2cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSnjlBZOkTJ0"
   },
   "outputs": [],
   "source": [
    "def get_download_model_command(file_id, file_name):\n",
    "    \"\"\" Get wget download command for downloading the desired model and save to directory ../checkpoint/. \"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    save_path = MODEL_DIR\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
    "    return url\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    \"encoder\": {\"id\": \"1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej\", \"name\": \"encoder.pt\"},\n",
    "    \"faceparsing\": {\"id\": \"1jY0mTjVB8njDh6e0LP_2UxuRK3MnjoIR\", \"name\": \"faceparsing.pth\"},\n",
    "    \"arcane_exstyle\": {\"id\": \"1TC67wRJkdmNRZTqYMUEFkrhWRKKZW40c\", \"name\": \"exstyle_code.npy\"},\n",
    "    \"caricature_exstyle\": {\"id\": \"1xr9sx_WmRYJ4qHGTtdVQCSxSo4HP3-ip\", \"name\": \"exstyle_code.npy\"},\n",
    "    \"cartoon_exstyle\": {\"id\": \"1BuCeLk3ASZcoHlbfT28qNru4r5f-hErr\", \"name\": \"exstyle_code.npy\"},\n",
    "    \"pixar_exstyle\": {\"id\": \"1yTaKuSrL7I0i0RYEEK5XD6GI-y5iNUbj\", \"name\": \"exstyle_code.npy\"},\n",
    "    \"arcane000\": {\"id\": \"1pF4fJ8acmawMsjjXo4HXRIOXeZR8jLVh\", \"name\": \"generator.pt\"},\n",
    "    \"arcane077\": {\"id\": \"16rLTF2oC0ZeurnM6hjrfrc8BxtW8P8Qf\", \"name\": \"generator.pt\"},\n",
    "    \"caricature039\": {\"id\": \"1C1E4WEoDWzl0nAxR9okKffFmlMOENbeF\", \"name\": \"generator.pt\"},\n",
    "    \"caricature068\": {\"id\": \"1B1ko1x8fX2aJ4BYCL12AnknVAi3qQc8W\", \"name\": \"generator.pt\"},\n",
    "    \"cartoon026\": {\"id\": \"1YJYODh_vEyUrL0q02okjcicpJhdYY8An\", \"name\": \"generator.pt\"},\n",
    "    \"cartoon299\": {\"id\": \"101qMUMfcI2qDxEbfCBt5mOg2aSqdTaIt\", \"name\": \"generator.pt\"},\n",
    "    \"pixar052\": {\"id\": \"16j_l1x0DD0PjwO8YdplAk69sh3-v95rr\", \"name\": \"generator.pt\"},\n",
    "    \"cartoon\": {\"id\": \"11s0hwhZWTLacMAzZH4OU-o3Qkp54h30J\", \"name\": \"generator.pt\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I - Style Transfer with specialized VToonify-D model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRjtz6uLkTJs"
   },
   "source": [
    "## Step 1: Select Style Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_types = ['cartoon026',      # balanced\n",
    "               'cartoon299',      # big eyes\n",
    "               'arcane000',       # for female\n",
    "               'arcane077',       # for male\n",
    "               'pixar052',\n",
    "               'caricature039',   # big mouth\n",
    "               'caricature068',   # balanced\n",
    "              ]\n",
    "style_type = style_types[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4etDz82xkTJz"
   },
   "source": [
    "## Step 2: Download Pretrained Models \n",
    "As part of this repository, we provide pretrained models. We'll download the model and save them to the folder `../checkpoint/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pSp encoder and face parsinf network\n",
    "path = MODEL_PATHS[\"encoder\"]\n",
    "download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
    "!{download_command}\n",
    "path = MODEL_PATHS[\"faceparsing\"]\n",
    "download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
    "!{download_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ31J_m7kTJ8"
   },
   "outputs": [],
   "source": [
    "# download vtoonify\n",
    "path = MODEL_PATHS[style_type]\n",
    "download_command = get_download_model_command(file_id=path[\"id\"], file_name = style_type + '_' + path[\"name\"])\n",
    "!{download_command}\n",
    "# download extrinsic style code\n",
    "path = MODEL_PATHS[style_type[:-3]+'_exstyle']\n",
    "download_command = get_download_model_command(file_id=path[\"id\"], file_name = style_type[:-3] + '_' + path[\"name\"])\n",
    "!{download_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAWrUehTkTKJ"
   },
   "source": [
    "## Step 3: Load Pretrained Model\n",
    "We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5,0.5,0.5]),\n",
    "    ])\n",
    "\n",
    "vtoonify = VToonify(backbone = 'dualstylegan')\n",
    "vtoonify.load_state_dict(torch.load(os.path.join(MODEL_DIR, style_type+'_generator.pt'), map_location=lambda storage, loc: storage)['g_ema'])\n",
    "vtoonify.to(device)\n",
    "\n",
    "parsingpredictor = BiSeNet(n_classes=19)\n",
    "parsingpredictor.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'faceparsing.pth'), map_location=lambda storage, loc: storage))\n",
    "parsingpredictor.to(device).eval()\n",
    "\n",
    "modelname = './checkpoint/shape_predictor_68_face_landmarks.dat'\n",
    "if not os.path.exists(modelname):\n",
    "    wget.download('http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2', modelname+'.bz2')\n",
    "    zipfile = bz2.BZ2File(modelname+'.bz2')\n",
    "    data = zipfile.read()\n",
    "    open(modelname, 'wb').write(data) \n",
    "landmarkpredictor = dlib.shape_predictor(modelname)\n",
    "\n",
    "pspencoder = load_psp_standalone(os.path.join(MODEL_DIR, 'encoder.pt'), device)    \n",
    "\n",
    "exstyles = np.load(os.path.join(MODEL_DIR, style_type[:-3]+'_exstyle_code.npy'), allow_pickle='TRUE').item()\n",
    "stylename = list(exstyles.keys())[int(style_type[-3:])]\n",
    "exstyle = torch.tensor(exstyles[stylename]).to(device)\n",
    "with torch.no_grad():  \n",
    "    exstyle = vtoonify.zplus2wplus(exstyle)\n",
    "    \n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4weLFoPbkTKZ"
   },
   "source": [
    "## Step 4: Image Toonification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6oqf8JwzK0K"
   },
   "source": [
    "### Visualize and Rescale Input\n",
    "We rescale the input image to make it fit our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2H9zFLJkTKa"
   },
   "outputs": [],
   "source": [
    "image_path = './data/077436.jpg'\n",
    "original_image = load_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "-lbLKtl-kTKc",
    "outputId": "9b5b83fb-233c-4cc2-905e-cc2d3949ccd5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=30)\n",
    "visualize(original_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJ9Ce1aYzmFF"
   },
   "outputs": [],
   "source": [
    "frame = cv2.imread(image_path)\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "scale = 1\n",
    "kernel_1d = np.array([[0.125],[0.375],[0.375],[0.125]])\n",
    "# We detect the face in the image, and resize the image so that the eye distance is 64 pixels.\n",
    "# Centered on the eyes, we crop the image to almost 400x400 (based on args.padding).\n",
    "paras = get_video_crop_parameter(frame, landmarkpredictor, padding=[200,200,200,200])\n",
    "if paras is not None:\n",
    "    h,w,top,bottom,left,right,scale = paras\n",
    "    H, W = int(bottom-top), int(right-left)\n",
    "    # for HR image, we apply gaussian blur to it to avoid over-sharp stylization results\n",
    "    if scale <= 0.75:\n",
    "        frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "    if scale <= 0.375:\n",
    "        frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "    frame = cv2.resize(frame, (w, h))[top:bottom, left:right]\n",
    "    x = transform(frame).unsqueeze(dim=0).to(device)\n",
    "else:\n",
    "    print('no face detected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "hUBAfodh5PaM",
    "outputId": "f44ef8fd-293d-4048-9fa7-848a7111f28a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=30)\n",
    "visualize(x[0].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0BmXzu1kTKg"
   },
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    I = align_face(frame, landmarkpredictor)\n",
    "    I = transform(I).unsqueeze(dim=0).to(device)\n",
    "    s_w = pspencoder(I)\n",
    "    s_w = vtoonify.zplus2wplus(s_w)\n",
    "    s_w[:,:7] = exstyle[:,:7]\n",
    "    # parsing network works best on 512x512 images, so we predict parsing maps on upsmapled frames\n",
    "    # followed by downsampling the parsing maps\n",
    "    x_p = F.interpolate(parsingpredictor(2*(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)))[0], \n",
    "                        scale_factor=0.5, recompute_scale_factor=False).detach()\n",
    "    # we give parsing maps lower weight (1/16)\n",
    "    inputs = torch.cat((x, x_p/16.), dim=1)\n",
    "    # d_s has no effect when backbone is toonify\n",
    "    y_tilde = vtoonify(inputs, s_w.repeat(inputs.size(0), 1, 1), d_s = 0.5)        \n",
    "    y_tilde = torch.clamp(y_tilde, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=60)\n",
    "visualize(y_tilde[0].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4weLFoPbkTKZ"
   },
   "source": [
    "## Step 5: Video Toonification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6oqf8JwzK0K"
   },
   "source": [
    "### Visualize and Rescale Input\n",
    "We rescale the input video to make it fit our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './data/529.mp4'\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "num = int(video_cap.get(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, frame = video_cap.read()\n",
    "if success == False:\n",
    "    assert('load video frames error')\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=30)\n",
    "visualize(transform(frame))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1\n",
    "kernel_1d = np.array([[0.125],[0.375],[0.375],[0.125]])\n",
    "# We proprocess the video by detecting the face in the first frame, \n",
    "# and resizing the frame so that the eye distance is 64 pixels.\n",
    "# Centered on the eyes, we crop the first frame to almost 400x400 (based on args.padding).\n",
    "# All other frames use the same resizing and cropping parameters as the first frame.\n",
    "paras = get_video_crop_parameter(frame, landmarkpredictor, padding=[200,200,200,200])\n",
    "if paras is None:\n",
    "    print('no face detected!')\n",
    "else:\n",
    "    h,w,top,bottom,left,right,scale = paras\n",
    "    H, W = int(bottom-top), int(right-left)\n",
    "# for HR video, we apply gaussian blur to the frames to avoid flickers caused by bilinear downsampling\n",
    "# this can also prevent over-sharp stylization results. \n",
    "if scale <= 0.75:\n",
    "    frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "if scale <= 0.375:\n",
    "    frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "frame = cv2.resize(frame, (w, h))[top:bottom, left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=30)\n",
    "visualize(transform(frame))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0BmXzu1kTKg"
   },
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "videoWriter = cv2.VideoWriter(os.path.join(OUT_DIR, 'result.mp4'), fourcc, video_cap.get(5), (4*W, 4*H))\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num)):\n",
    "        if i == 0:        \n",
    "            I = align_face(frame, landmarkpredictor)\n",
    "            I = transform(I).unsqueeze(dim=0).to(device)\n",
    "            s_w = pspencoder(I)\n",
    "            s_w = vtoonify.zplus2wplus(s_w)\n",
    "            s_w[:,:7] = exstyle[:,:7]\n",
    "        else\n",
    "            if scale <= 0.75:\n",
    "                frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "            if scale <= 0.375:\n",
    "                frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "            frame = cv2.resize(frame, (w, h))[top:bottom, left:right]\n",
    "\n",
    "        batch_frames += [transform(frame).unsqueeze(dim=0).to(device)]\n",
    "\n",
    "        if len(batch_frames) == batch_size or (i+1) == num:\n",
    "            x = torch.cat(batch_frames, dim=0)\n",
    "            batch_frames = []\n",
    "            # parsing network works best on 512x512 images, so we predict parsing maps on upsmapled frames\n",
    "            # followed by downsampling the parsing maps\n",
    "            x_p = F.interpolate(parsingpredictor(2*(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)))[0], \n",
    "                            scale_factor=0.5, recompute_scale_factor=False).detach()\n",
    "            # we give parsing maps lower weight (1/16)\n",
    "            inputs = torch.cat((x, x_p/16.), dim=1)\n",
    "            # d_s has no effect when backbone is toonify\n",
    "            y_tilde = vtoonify(inputs, s_w.repeat(inputs.size(0), 1, 1), d_s = args.style_degree)       \n",
    "            y_tilde = torch.clamp(y_tilde, -1, 1)\n",
    "            for k in range(y_tilde.size(0)):\n",
    "                videoWriter.write(tensor2cv2(y_tilde[k].cpu()))\n",
    "videoWriter.release()\n",
    "video_cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = torchvision.utils.make_grid(y_tilde, 2, 2)\n",
    "plt.figure(figsize=(10,10),dpi=120)\n",
    "visualize(viz.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the stylized video in `./output/result.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II - Style control with VToonify-Dsd model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4etDz82xkTJz"
   },
   "source": [
    "## Step 1: Download Pretrained Models \n",
    "As part of this repository, we provide pretrained models. We'll download the model and save them to the folder `../checkpoint/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you haved downloaded the encoder and faceparsing model in PART I, skip this step\n",
    "if False:\n",
    "    path = MODEL_PATHS[\"encoder\"]\n",
    "    download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
    "    !{download_command}\n",
    "    path = MODEL_PATHS[\"faceparsing\"]\n",
    "    download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
    "    !{download_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ31J_m7kTJ8"
   },
   "outputs": [],
   "source": [
    "# download the style code and the vtoonify-Dsd\n",
    "path = MODEL_PATHS['cartoon_exstyle']\n",
    "download_command = get_download_model_command(file_id=path[\"id\"], file_name = 'cartoon_cartoon_exstyle.npy')\n",
    "!{download_command}\n",
    "path = MODEL_PATHS['cartoon']\n",
    "download_command = get_download_model_command(file_id=path[\"id\"], file_name = 'cartoon_generator.pt')\n",
    "!{download_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAWrUehTkTKJ"
   },
   "source": [
    "## Step 2: Load Pretrained Model\n",
    "We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you haved load the models in PART I, skip this step, or set False to True\n",
    "if False:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5,0.5,0.5]),\n",
    "        ])\n",
    "\n",
    "    parsingpredictor = BiSeNet(n_classes=19)\n",
    "    parsingpredictor.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'faceparsing.pth'), map_location=lambda storage, loc: storage))\n",
    "    parsingpredictor.to(device).eval()\n",
    "\n",
    "    modelname = './checkpoint/shape_predictor_68_face_landmarks.dat'\n",
    "    if not os.path.exists(modelname):\n",
    "        wget.download('http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2', modelname+'.bz2')\n",
    "        zipfile = bz2.BZ2File(modelname+'.bz2')\n",
    "        data = zipfile.read()\n",
    "        open(modelname, 'wb').write(data) \n",
    "    landmarkpredictor = dlib.shape_predictor(modelname)\n",
    "\n",
    "    pspencoder = load_psp_standalone(os.path.join(MODEL_DIR, 'encoder.pt'), device)    \n",
    "    \n",
    "vtoonify = VToonify(backbone = 'dualstylegan')\n",
    "vtoonify.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'cartoon_generator.pt'), map_location=lambda storage, loc: storage)['g_ema'])\n",
    "vtoonify.to(device)\n",
    "\n",
    "exstyles = np.load(os.path.join(MODEL_DIR, 'cartoon_cartoon_exstyle.npy'), allow_pickle='TRUE').item()  \n",
    "styles = []\n",
    "with torch.no_grad(): \n",
    "    for stylename in exstyles.keys():\n",
    "        exstyle = torch.tensor(exstyles[stylename]).to(device)\n",
    "        exstyle = vtoonify.zplus2wplus(exstyle)\n",
    "        styles += [exstyle]\n",
    "exstyles = torch.cat(styles, dim=0)\n",
    "    \n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4weLFoPbkTKZ"
   },
   "source": [
    "## Step 3: Image Toonification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6oqf8JwzK0K"
   },
   "source": [
    "### Visualize and Rescale Input\n",
    "We rescale the input image to make it fit our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2H9zFLJkTKa"
   },
   "outputs": [],
   "source": [
    "image_path = './data/077436.jpg'\n",
    "original_image = load_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "-lbLKtl-kTKc",
    "outputId": "9b5b83fb-233c-4cc2-905e-cc2d3949ccd5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=30)\n",
    "visualize(original_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJ9Ce1aYzmFF"
   },
   "outputs": [],
   "source": [
    "frame = cv2.imread(image_path)\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "scale = 1\n",
    "kernel_1d = np.array([[0.125],[0.375],[0.375],[0.125]])\n",
    "# We detect the face in the image, and resize the image so that the eye distance is 64 pixels.\n",
    "# Centered on the eyes, we crop the image to almost 400x400 (based on args.padding).\n",
    "paras = get_video_crop_parameter(frame, landmarkpredictor, padding=[200,200,200,200])\n",
    "if paras is not None:\n",
    "    h,w,top,bottom,left,right,scale = paras\n",
    "    H, W = int(bottom-top), int(right-left)\n",
    "    # for HR image, we apply gaussian blur to it to avoid over-sharp stylization results\n",
    "    if scale <= 0.75:\n",
    "        frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "    if scale <= 0.375:\n",
    "        frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
    "    frame = cv2.resize(frame, (w, h))[top:bottom, left:right]\n",
    "    x = transform(frame).unsqueeze(dim=0).to(device)\n",
    "else:\n",
    "    print('no face detected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "hUBAfodh5PaM",
    "outputId": "f44ef8fd-293d-4048-9fa7-848a7111f28a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10),dpi=30)\n",
    "visualize(x[0].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select style image\n",
    "\n",
    "Select the style index (the mapping between index and style image is defined [here](https://github.com/williamyang1991/DualStyleGAN/blob/main/doc_images/cartoon_overview.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_id = [8, 26, 64, 153, 299]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style transfer with different cartoon structure styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5POMJ5YkTKl"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    I = align_face(frame, landmarkpredictor)\n",
    "    I = transform(I).unsqueeze(dim=0).to(device)\n",
    "    s_w = pspencoder(I)\n",
    "    s_w = vtoonify.zplus2wplus(s_w).repeat(len(style_id), 1, 1)\n",
    "    s_w[:,:7] = exstyles[style_id,:7]\n",
    "    x = x.repeat(len(style_id), 1, 1, 1)\n",
    "    # parsing network works best on 512x512 images, so we predict parsing maps on upsmapled frames\n",
    "    # followed by downsampling the parsing maps\n",
    "    x_p = F.interpolate(parsingpredictor(2*(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)))[0], \n",
    "                        scale_factor=0.5, recompute_scale_factor=False).detach()\n",
    "    # we give parsing maps lower weight (1/16)\n",
    "    inputs = torch.cat((x, x_p/16.), dim=1)\n",
    "    # d_s has no effect when backbone is toonify\n",
    "    y_tilde = vtoonify(inputs, s_w.repeat(inputs.size(0), 1, 1), d_s = 0.6)        \n",
    "    y_tilde = torch.clamp(y_tilde, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = torchvision.utils.make_grid(y_tilde, 2, 2)\n",
    "plt.figure(figsize=(10,10),dpi=120)\n",
    "visualize(viz.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation with different style degree to achieve flexible style manipulation\n",
    "\n",
    "Users are suggested to try different style degrees to find the ideal results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(5):\n",
    "    d_s = i / 4.0\n",
    "    y_tilde = vtoonify(inputs, s_w.repeat(inputs.size(0), 1, 1), d_s = d_s)  \n",
    "    y_tilde = torch.clamp(y_tilde, -1, 1)\n",
    "    results += [y_tilde]\n",
    "        \n",
    "vis = torchvision.utils.make_grid(torch.cat(y_tilde, dim=0), 5, 1)\n",
    "plt.figure(figsize=(10,10),dpi=120)\n",
    "visualize(vis.cpu())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "inference_playground.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "psp_env",
   "language": "python",
   "name": "psp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
